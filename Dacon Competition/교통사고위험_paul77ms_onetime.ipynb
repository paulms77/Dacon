{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Process\n",
        "\n",
        "KFold\n",
        "  - DeepFM\n",
        "\n",
        "  - AutoML\n",
        "\n",
        "  - Trasnformer"
      ],
      "metadata": {
        "id": "4G5Z051UUxDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mljar-supervised"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iI_NlSkKVMSf",
        "outputId": "2ac0fa87-ad64-4667-9de7-eb8a24d75667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mljar-supervised\n",
            "  Downloading mljar-supervised-1.1.1.tar.gz (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.9/126.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from mljar-supervised) (1.23.5)\n",
            "Collecting pandas>=2.0.0 (from mljar-supervised)\n",
            "  Using cached pandas-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "Requirement already satisfied: scipy>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from mljar-supervised) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.10/dist-packages (from mljar-supervised) (1.2.2)\n",
            "Requirement already satisfied: xgboost>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from mljar-supervised) (2.0.2)\n",
            "Requirement already satisfied: lightgbm>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from mljar-supervised) (4.1.0)\n",
            "Collecting catboost>=0.24.4 (from mljar-supervised)\n",
            "  Using cached catboost-1.2.2-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
            "Requirement already satisfied: joblib>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from mljar-supervised) (1.3.2)\n",
            "Requirement already satisfied: tabulate>=0.8.7 in /usr/local/lib/python3.10/dist-packages (from mljar-supervised) (0.9.0)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from mljar-supervised) (3.7.1)\n",
            "Collecting dtreeviz>=2.2.2 (from mljar-supervised)\n",
            "  Downloading dtreeviz-2.2.2-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting shap>=0.42.1 (from mljar-supervised)\n",
            "  Downloading shap-0.44.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (533 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m533.5/533.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: seaborn>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from mljar-supervised) (0.12.2)\n",
            "Requirement already satisfied: wordcloud>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from mljar-supervised) (1.9.2)\n",
            "Collecting category_encoders>=2.2.2 (from mljar-supervised)\n",
            "  Downloading category_encoders-2.6.3-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting optuna>=2.7.0 (from mljar-supervised)\n",
            "  Downloading optuna-3.4.0-py3-none-any.whl (409 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.6/409.6 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-plot==0.3.7 (from mljar-supervised)\n",
            "  Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from mljar-supervised) (3.5.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from mljar-supervised) (4.5.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from mljar-supervised) (7.34.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost>=0.24.4->mljar-supervised) (0.20.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost>=0.24.4->mljar-supervised) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost>=0.24.4->mljar-supervised) (1.16.0)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders>=2.2.2->mljar-supervised) (0.14.0)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category_encoders>=2.2.2->mljar-supervised) (0.5.3)\n",
            "Requirement already satisfied: colour in /usr/local/lib/python3.10/dist-packages (from dtreeviz>=2.2.2->mljar-supervised) (0.1.5)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from dtreeviz>=2.2.2->mljar-supervised) (7.4.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->mljar-supervised) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->mljar-supervised) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->mljar-supervised) (4.45.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->mljar-supervised) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->mljar-supervised) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->mljar-supervised) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->mljar-supervised) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->mljar-supervised) (2.8.2)\n",
            "Collecting alembic>=1.5.0 (from optuna>=2.7.0->mljar-supervised)\n",
            "  Downloading alembic-1.13.0-py3-none-any.whl (230 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.6/230.6 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna>=2.7.0->mljar-supervised)\n",
            "  Downloading colorlog-6.8.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=2.7.0->mljar-supervised) (2.0.23)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna>=2.7.0->mljar-supervised) (4.66.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna>=2.7.0->mljar-supervised) (6.0.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->mljar-supervised) (2023.3.post1)\n",
            "Collecting tzdata>=2022.1 (from pandas>=2.0.0->mljar-supervised)\n",
            "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->mljar-supervised) (3.2.0)\n",
            "Collecting slicer==0.0.7 (from shap>=0.42.1->mljar-supervised)\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap>=0.42.1->mljar-supervised) (0.58.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap>=0.42.1->mljar-supervised) (2.2.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->mljar-supervised) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->mljar-supervised)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->mljar-supervised) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->mljar-supervised) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->mljar-supervised) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->mljar-supervised) (3.0.41)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->mljar-supervised) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->mljar-supervised) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->mljar-supervised) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->mljar-supervised) (4.9.0)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna>=2.7.0->mljar-supervised)\n",
            "  Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->mljar-supervised) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->mljar-supervised) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->mljar-supervised) (0.2.12)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna>=2.7.0->mljar-supervised) (3.0.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap>=0.42.1->mljar-supervised) (0.41.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost>=0.24.4->mljar-supervised) (8.2.3)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->dtreeviz>=2.2.2->mljar-supervised) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->dtreeviz>=2.2.2->mljar-supervised) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->dtreeviz>=2.2.2->mljar-supervised) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->dtreeviz>=2.2.2->mljar-supervised) (2.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna>=2.7.0->mljar-supervised) (2.1.3)\n",
            "Building wheels for collected packages: mljar-supervised\n",
            "  Building wheel for mljar-supervised (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mljar-supervised: filename=mljar_supervised-1.1.1-py3-none-any.whl size=164970 sha256=bdd03c3d5cd459a2648b9a5dd1b2ebc997626627733ef90112068fbdb22578d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/66/9f/bdde96c1acfe3870621cef302d8e41b5d507a4d940ce7ba16e\n",
            "Successfully built mljar-supervised\n",
            "Installing collected packages: tzdata, slicer, Mako, jedi, colorlog, pandas, alembic, shap, scikit-plot, optuna, dtreeviz, catboost, category_encoders, mljar-supervised\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.1.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Mako-1.3.0 alembic-1.13.0 catboost-1.2.2 category_encoders-2.6.3 colorlog-6.8.0 dtreeviz-2.2.2 jedi-0.19.1 mljar-supervised-1.1.1 optuna-3.4.0 pandas-2.1.4 scikit-plot-0.3.7 shap-0.44.0 slicer-0.0.7 tzdata-2023.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade autogluon"
      ],
      "metadata": {
        "id": "C5WXasS3HXPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from category_encoders.target_encoder import TargetEncoder\n",
        "import warnings\n",
        "from supervised import AutoML\n",
        "from google.colab import drive\n",
        "import datetime as dt\n",
        "import zipfile\n",
        "#from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "#from autogluon.core.metrics import make_scorer\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "T9641BFGUmyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzzyFHwBW5Xr",
        "outputId": "63fc4a63-a6f1-4d0c-8866-ecf7687202a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive_maindir = '/content/drive/MyDrive/'\n",
        "#os.mkdir(drive_maindir+'danger')\n",
        "drive_zipmaindir = '/content/drive/MyDrive/open.zip'"
      ],
      "metadata": {
        "id": "REKioU9NWetb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# zipfile = zipfile.ZipFile(drive_zipmaindir)\n",
        "# zipfile.extractall(path = drive_maindir+'danger')"
      ],
      "metadata": {
        "id": "2q_yaMA1WtP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive_dangerdir = drive_maindir+'danger/open/'\n",
        "train = pd.read_csv(drive_dangerdir + 'train.csv')\n",
        "test = pd.read_csv(drive_dangerdir + 'test.csv')\n",
        "sample_submission = pd.read_csv(drive_dangerdir + 'sample_submission.csv')"
      ],
      "metadata": {
        "id": "BWByca2QYAP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 도로유형 사고 위험도 관계\n",
        "# ex) 국도, 지방도, 고속도로 등"
      ],
      "metadata": {
        "id": "5FWcT-T4S81b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train[['ID', '사고일시', '요일', '기상상태', '시군구', '도로형태', '노면상태', '사고유형', 'ECLO']]"
      ],
      "metadata": {
        "id": "nPNuM_R-aMTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['사고일시'] = pd.to_datetime(train['사고일시'])\n",
        "test['사고일시'] = pd.to_datetime(test['사고일시'])"
      ],
      "metadata": {
        "id": "kUaiBj_AYAJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['도시'] = train['시군구'].str.split(' ').str[0]\n",
        "train['구'] = train['시군구'].str.split(' ').str[1]\n",
        "train['동'] = train['시군구'].str.split(' ').str[2]\n",
        "test['도시'] = test['시군구'].str.split(' ').str[0]\n",
        "test['구'] = test['시군구'].str.split(' ').str[1]\n",
        "test['동'] = test['시군구'].str.split(' ').str[2]\n",
        "\n",
        "train['사고일시'] = pd.to_datetime(train['사고일시'])\n",
        "train['사고연도'] = train['사고일시'].dt.year.astype(int)\n",
        "train['사고월'] = train['사고일시'].dt.month.astype(int)\n",
        "train['사고일'] = train['사고일시'].dt.day.astype(int)\n",
        "train['사고요일'] = train['사고일시'].dt.dayofweek.astype(int)\n",
        "train['사고시간'] = train['사고일시'].dt.hour.astype(int)\n",
        "train.drop(['사고일시'], axis = 1, inplace = True)\n",
        "test['사고일시'] = pd.to_datetime(test['사고일시'])\n",
        "test['사고연도'] = test['사고일시'].dt.year.astype(int)\n",
        "test['사고월'] = test['사고일시'].dt.month.astype(int)\n",
        "test['사고일'] = test['사고일시'].dt.day.astype(int)\n",
        "test['사고요일'] = test['사고일시'].dt.dayofweek.astype(int)\n",
        "test['사고시간'] = test['사고일시'].dt.hour.astype(int)\n",
        "test.drop(['사고일시'], axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "Bv5GCYdFYX8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get1(df):\n",
        "  df['sin_사고시간'] = np.sin(2*np.pi*df['사고시간']/24)\n",
        "  df['cos_사고시간'] = np.cos(2*np.pi*df['사고시간']/24)\n",
        "  return df\n",
        "\n",
        "train = get1(train)\n",
        "test = get1(test)\n",
        "\n",
        "# 계절\n",
        "def get2(df):\n",
        "  df['계절'] = np.nan\n",
        "  df.loc[(df['사고월'] == 3) | (df['사고월'] == 4) | (df['사고월'] == 5), '계절'] = '봄'\n",
        "  df.loc[(df['사고월'] == 6) | (df['사고월'] == 7) | (df['사고월'] == 8), '계절'] = '여름'\n",
        "  df.loc[(df['사고월'] == 9) | (df['사고월'] == 10) | (df['사고월'] == 11), '계절'] = '가을'\n",
        "  df.loc[(df['사고월'] == 12) | (df['사고월'] == 1) | (df['사고월'] == 2), '계절'] = '겨울'\n",
        "  return df['계절']\n",
        "\n",
        "train['계절'] = get2(train)\n",
        "test['계절'] = get2(test)\n",
        "\n",
        "# 주말여부\n",
        "def get3(df):\n",
        "  df['사고주말여부'] = np.nan\n",
        "  df.loc[(df['사고요일'] < 5), '사고주말여부'] = 0\n",
        "  df.loc[(df['사고요일'] >= 5), '사고주말여부'] = 1\n",
        "  return df['사고주말여부']\n",
        "\n",
        "train['사고주말여부'] = get3(train)\n",
        "test['사고주말여부'] = get3(test)\n",
        "\n",
        "# 시간대\n",
        "def get4(df):\n",
        "  df['시간대'] = np.nan\n",
        "  df.loc[(df['사고시간'] < 6), '시간대'] = '새벽'\n",
        "  df.loc[(df['사고시간'] >=6) & (df['사고시간'] < 12), '시간대'] = '아침'\n",
        "  df.loc[(df['사고시간'] >= 12) & (df['사고시간'] < 19), '시간대'] = '오후'\n",
        "  df.loc[(df['사고시간'] >= 19) & (df['사고시간'] <= 24), '시간대'] = '저녁'\n",
        "  return df['시간대']\n",
        "\n",
        "train['시간대'] = get4(train)\n",
        "test['시간대'] = get4(test)\n",
        "\n",
        "# 공휴일\n",
        "\n",
        "# 공휴일 전\n",
        "\n",
        "# 공휴일 후"
      ],
      "metadata": {
        "id": "4FBJsgYocBQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "외부 데이터"
      ],
      "metadata": {
        "id": "jMdQqaQbZliX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive_dangerexternaldir = drive_dangerdir+'external_open/'\n",
        "\n",
        "# 대구 빅데이터 마트 정보\n",
        "#\n",
        "\n",
        "external_data2 = pd.read_csv(drive_dangerexternaldir + 'countrywide_accident.csv') # equal = train\n",
        "external_data3 = pd.read_csv(drive_dangerexternaldir + '대구 CCTV 정보.csv', encoding = 'cp949') # check\n",
        "external_data4 = pd.read_csv(drive_dangerexternaldir + '대구 보안등 정보.csv', encoding = 'cp949') # check\n",
        "external_data5 = pd.read_csv(drive_dangerexternaldir + '대구 어린이 보호 구역 정보.csv', encoding = 'cp949') # check\n",
        "external_data6 = pd.read_csv(drive_dangerexternaldir + '대구 주차장 정보.csv', encoding = 'cp949') #check"
      ],
      "metadata": {
        "id": "rtfNePuxZKxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_location_pattern(df, address_column, drop_columns):\n",
        "  location_pattern = r'(\\S+) (\\S+) (\\S+) (\\S+)'\n",
        "  df[['도시', '구', '동', '번지']] = df[address_column].str.extract(location_pattern)\n",
        "  df.drop(columns = drop_columns, axis = 1, inplace = True)\n",
        "  return df"
      ],
      "metadata": {
        "id": "oPFi9AGKf4B9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CCTV\n",
        "# 도로노선방향, 위도, 경도, 단속구분, 제한속도, 설치연도\n",
        "external_data3 = process_location_pattern(external_data3, '소재지지번주소', ['번지', '소재지지번주소'])\n",
        "external_data3_grouped = external_data3.groupby(['도시', '구', '동'])[['도로노선방향', '위도', '경도', '단속구분', '제한속도', '설치연도']].mean().reset_index()\n",
        "external_data3_grouped.rename(columns={'위도': 'CCTV_위도', '경도': 'CCTV_경도'}, inplace=True)\n",
        "external_data3_grouped.reset_index(inplace = True, drop = True)\n",
        "\n",
        "# (설치연도 - 사고일시)?"
      ],
      "metadata": {
        "id": "6PD384WahzFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "external_data3_notnull = external_data3.dropna(subset = ['도시', '구', '동'])\n",
        "cctv_count = external_data3_notnull.groupby(['도시', '구', '동']).size().reset_index(name = 'CCTV개수')"
      ],
      "metadata": {
        "id": "mwBWmNgYOqUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 보안등\n",
        "# 위도, 경도, 설치연도, 설치형태, 설치개수\n",
        "\n",
        "external_data4 = process_location_pattern(external_data4, '소재지지번주소', ['번지', '소재지지번주소'])\n",
        "external_data4 = pd.get_dummies(external_data4, columns = ['설치형태'], drop_first = True)\n",
        "\n",
        "external_data4_grouped1 = external_data4.groupby(['도시', '구', '동'])[['위도', '경도', '설치연도']].mean().reset_index()\n",
        "external_data4_grouped1.rename(columns={'위도': '보안등_위도', '경도': '보안등_경도'}, inplace=True)\n",
        "\n",
        "external_data4_grouped2 = external_data4.groupby(['도시', '구', '동'])[external_data4.columns[external_data4.columns.str.contains('설치형태')]].sum().reset_index()\n",
        "\n",
        "external_data4_grouped3 = external_data4.groupby(['도시', '구', '동'])['설치개수'].sum().reset_index()\n",
        "\n",
        "external_data4_grouped = pd.merge(external_data4_grouped1, external_data4_grouped2, on = ['도시', '구', '동'], how = 'left')\n",
        "external_data4_grouped = pd.merge(external_data4_grouped, external_data4_grouped3, on = ['도시', '구', '동'], how = 'left')\n",
        "external_data4_grouped.reset_index(inplace = True, drop = True)"
      ],
      "metadata": {
        "id": "F033CS_5dPIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_average(value_str):\n",
        "  if '~' in value_str:\n",
        "    start, end = map(float, value_str.split('~'))\n",
        "    return (start + end) / 2\n",
        "  if '-' in value_str:\n",
        "    start, end = map(float, value_str.split('-'))\n",
        "    return (start + end) / 2\n",
        "  else:\n",
        "    return float(value_str)\n",
        "\n",
        "external_data5['보호구역도로폭'] = external_data5['보호구역도로폭'].astype(str).apply(calculate_average)"
      ],
      "metadata": {
        "id": "2GRKIaYppwDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 어린이 보호 구역\n",
        "# 소재지지번주소, 위도, 경도, CCTV설치대수, 보호구역도로폭\n",
        "\n",
        "external_data5 = process_location_pattern(external_data5, '소재지지번주소', ['번지', '소재지지번주소'])\n",
        "\n",
        "external_data5_grouped1 = external_data5.groupby(['도시', '구', '동'])[['위도', '경도', '보호구역도로폭']].mean().reset_index()\n",
        "external_data5_grouped1.rename(columns={'위도': '어린이보호구역_위도', '경도': '어린이보호구역_경도'}, inplace=True)\n",
        "\n",
        "external_data5_grouped2 = external_data5.groupby(['도시', '구', '동'])['CCTV설치대수'].sum().reset_index()\n",
        "\n",
        "external_data5_grouped = pd.merge(external_data5_grouped1, external_data5_grouped2, on = ['도시', '구', '동'], how = 'left')\n",
        "external_data5_grouped.reset_index(inplace = True, drop = True)"
      ],
      "metadata": {
        "id": "m5K-5Vjhev_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 초등학교 개수\n",
        "external_data5_n = external_data5.groupby(['도시', '구', '동'])['대상시설명'].nunique().reset_index()\n",
        "external_data5_n.columns = ['도시', '구', '동', '초등학교개수']"
      ],
      "metadata": {
        "id": "yAeedKtfRlsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 주차장\n",
        "external_data6 = process_location_pattern(external_data6, '소재지지번주소', ['번지', '소재지지번주소'])\n",
        "\n",
        "external_data6_grouped = external_data6.groupby(['도시', '구', '동'])[['위도', '경도']].mean().reset_index()\n",
        "external_data6_grouped.rename(columns={'위도': '주차장_위도', '경도': '주차장_경도'}, inplace=True)\n",
        "\n",
        "\n",
        "# 급지구분\n",
        "external_data6_p = pd.get_dummies(external_data6[['도시', '구', '동', '급지구분']], columns = ['급지구분'], prefix = '급지')\n",
        "\n",
        "# 주차장 개수\n",
        "external_data6_n = external_data6_p.groupby(['도시', '구', '동']).size().reset_index()\n",
        "external_data6_n.rename(columns = {0: '주차장개수'}, inplace = True)\n",
        "\n",
        "# 주차장 유형 개수\n",
        "external_data6_park = pd.get_dummies(external_data6[['도시', '구', '동', '주차장유형']], columns = ['주차장유형'], prefix = '주차장유형')\n",
        "external_data6_park_n = external_data6_park.groupby(['도시', '구', '동'])[external_data6_park.columns[external_data6_park.columns.str.contains('주차장유형')]].sum().reset_index()\n",
        "\n",
        "external_data6_sum = pd.merge(external_data6_n, external_data6_p, on = ['도시', '구', '동'], how = 'left')\n",
        "external_data6_sum = external_data6_sum.groupby(['도시', '구', '동']).sum().reset_index()\n",
        "external_data6_sum = pd.merge(external_data6_sum, external_data6_grouped, on = ['도시', '구', '동'], how = 'left')\n",
        "external_data6_sum = pd.merge(external_data6_sum, external_data6_park_n, on = ['도시', '구', '동'], how = 'left')"
      ],
      "metadata": {
        "id": "yLiHqoTQsMNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.merge(train, external_data3_grouped, on = ['도시', '구', '동'], how = 'left')\n",
        "train = pd.merge(train, cctv_count, on = ['도시', '구', '동'], how = 'left')\n",
        "train = pd.merge(train, external_data4_grouped, on = ['도시', '구', '동'], how = 'left')\n",
        "train = pd.merge(train, external_data5_grouped, on = ['도시', '구', '동'], how = 'left')\n",
        "train = pd.merge(train, external_data5_n, on = ['도시', '구', '동'], how = 'left')\n",
        "train = pd.merge(train, external_data6_sum, on = ['도시', '구', '동'], how = 'left')\n",
        "\n",
        "test = pd.merge(test, external_data3_grouped, on = ['도시', '구', '동'], how = 'left')\n",
        "test = pd.merge(test, cctv_count, on = ['도시', '구', '동'], how = 'left')\n",
        "test = pd.merge(test, external_data4_grouped, on = ['도시', '구', '동'], how = 'left')\n",
        "test = pd.merge(test, external_data5_grouped, on = ['도시', '구', '동'], how = 'left')\n",
        "test = pd.merge(test, external_data5_n, on = ['도시', '구', '동'], how = 'left')\n",
        "test = pd.merge(test, external_data6_sum, on = ['도시', '구', '동'], how = 'left')"
      ],
      "metadata": {
        "id": "WtdZ20bvRdCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train = pd.get_dummies(data = train, columns = ['요일', '기상상태', '도로형태', '노면상태', '사고유형'])\n",
        "train.drop(['도시'], axis = 1, inplace = True)\n",
        "# test = pd.get_dummies(data = test, columns = ['요일', '기상상태', '도로형태', '노면상태', '사고유형'])\n",
        "test.drop(['도시'], axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "UMWUGhIMVw7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_col = list(train.columns)\n",
        "test_col = list(test.columns)\n",
        "list(set(train_col) - set(test_col))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE0UH2fkWUVI",
        "outputId": "bac39d03-23c1-460b-cd54-f075cca79371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ECLO']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "foggy = [0] * len(test)\n",
        "test['기상상태_안개'] = foggy"
      ],
      "metadata": {
        "id": "vIjOmMC3WVx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.isnull().sum()[train.isnull().sum() > 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQX3r5E8Wl8D",
        "outputId": "a41cc453-3b32-473b-8e65-10022cf69a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "도로노선방향         1576\n",
              "CCTV_위도        1576\n",
              "CCTV_경도        1576\n",
              "단속구분           1576\n",
              "제한속도           1576\n",
              "설치연도_x         1576\n",
              "CCTV개수         1576\n",
              "보안등_위도        13065\n",
              "보안등_경도        13065\n",
              "설치연도_y        23629\n",
              "설치형태_전용주       9513\n",
              "설치형태_한전주       9513\n",
              "설치개수           9513\n",
              "어린이보호구역_위도    18426\n",
              "어린이보호구역_경도    18426\n",
              "보호구역도로폭       26966\n",
              "CCTV설치대수      18426\n",
              "초등학교개수        18426\n",
              "주차장개수          6543\n",
              "급지_1           6543\n",
              "급지_2           6543\n",
              "급지_3           6543\n",
              "주차장_위도        12231\n",
              "주차장_경도        12231\n",
              "주차장유형_노상       6543\n",
              "주차장유형_노외       6543\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test.isnull().sum()[test.isnull().sum() > 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07aYJm6NYeG-",
        "outputId": "8a2dd1ef-29cc-4386-cfec-f1ba2f81394b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "도로노선방향         455\n",
              "CCTV_위도        455\n",
              "CCTV_경도        455\n",
              "단속구분           455\n",
              "제한속도           455\n",
              "설치연도_x         455\n",
              "CCTV개수         455\n",
              "보안등_위도        3795\n",
              "보안등_경도        3795\n",
              "설치연도_y        6722\n",
              "설치형태_전용주      2771\n",
              "설치형태_한전주      2771\n",
              "설치개수          2771\n",
              "어린이보호구역_위도    4961\n",
              "어린이보호구역_경도    4961\n",
              "보호구역도로폭       7465\n",
              "CCTV설치대수      4961\n",
              "초등학교개수        4961\n",
              "주차장개수         1928\n",
              "급지_1          1928\n",
              "급지_2          1928\n",
              "급지_3          1928\n",
              "주차장_위도        3510\n",
              "주차장_경도        3510\n",
              "주차장유형_노상      1928\n",
              "주차장유형_노외      1928\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install haversine"
      ],
      "metadata": {
        "id": "OSWnoSsSO63O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "#from haversine import haversine\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "QNvi4SR-Mn9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 위도, 경도 클러스터링\n",
        "# # group0, group1 <-> group2\n",
        "# # group1 <-> group3\n",
        "\n",
        "group0 = ['CCTV_위도', 'CCTV_경도']\n",
        "group1 = ['보안등_위도', '보안등_경도']\n",
        "group2 = ['어린이보호구역_위도', '어린이보호구역_경도']\n",
        "group3 = ['주차장_위도', '주차장_경도']\n",
        "\n",
        "# def kmeans_best_k(df, group):\n",
        "#   silhouette_scores = []\n",
        "#   for k in range(1, 13):\n",
        "#       kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "#       labels = kmeans.fit_predict(df[group])\n",
        "#       silhouette_scores.append(silhouette_score(df[group], labels))\n",
        "\n",
        "#   plt.plot(range(1, 13), silhouette_scores, marker='o')\n",
        "#   plt.xlabel('Number of Clusters (k)')\n",
        "#   plt.ylabel('Silhouette Score')\n",
        "#   plt.title('Silhouette Score Method')\n",
        "#   plt.show()\n",
        "\n",
        "# kmeans_best_k(train, group0)\n",
        "# kmeans_best_k(train, group1)\n",
        "# kmeans_best_k(train, group2)\n",
        "# kmeans_best_k(train, group3)"
      ],
      "metadata": {
        "id": "mPoaI0OYYhUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 거리\n",
        "\n",
        "# def get_dist(df, group_s, group_e):\n",
        "#   start = tuple(zip(df[group_s[0]], df[group_s[1]]))\n",
        "#   end = tuple(zip(df[group_e[0]], df[group_e[1]]))\n",
        "#   dist = [haversine(s, e) for s, e in zip(start, end)]\n",
        "#   return dist\n",
        "\n",
        "# train['CCTV_어린이보호구역_거리'] = get_dist(train, group0, group2)\n",
        "# train['보안등_어린이보호구역_거리'] = get_dist(train, group1, group2)\n",
        "# train['보안등_주차장_거리'] = get_dist(train, group1, group3)\n",
        "# train['주차장_어린이보호구역_거리'] = get_dist(train, group3, group2)\n",
        "\n",
        "# test['CCTV_어린이보호구역_거리'] = get_dist(test, group0, group2)\n",
        "# test['보안등_어린이보호구역_거리'] = get_dist(test, group1, group2)\n",
        "# test['보안등_주차장_거리'] = get_dist(test, group1, group3)\n",
        "# test['주차장_어린이보호구역_거리'] = get_dist(test, group3, group2)"
      ],
      "metadata": {
        "id": "duZKiLXWQq3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "hPd4nU_LUUQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "WSAKOmF7Vl98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmsle(y, pred):\n",
        "  log_y = np.log1p(y)\n",
        "  log_pred = np.log1p(pred)\n",
        "  squared_error = (log_y-log_pred)**2\n",
        "  rmsle = np.sqrt(np.mean(squared_error))\n",
        "  return rmsle\n",
        "\n",
        "def rmse(y, pred):\n",
        "  return mean_squared_error(y, pred)**0.5"
      ],
      "metadata": {
        "id": "GGXHbPsLVlD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed: int = 42):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  os.environ[\"PYTHONHASHSEED\"] = str(seed)"
      ],
      "metadata": {
        "id": "Xl-2rV0SXW7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhASDDRcc5f-",
        "outputId": "0d47cf01-127d-4658-e20f-4a6dd087f4b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['ID', '요일', '기상상태', '시군구', '도로형태', '노면상태', '사고유형', 'ECLO', '구', '동',\n",
              "       '사고연도', '사고월', '사고일', '사고요일', '사고시간', 'sin_사고시간', 'cos_사고시간', '계절',\n",
              "       '사고주말여부', '시간대', '도로노선방향', 'CCTV_위도', 'CCTV_경도', '단속구분', '제한속도',\n",
              "       '설치연도_x', 'CCTV개수', '보안등_위도', '보안등_경도', '설치연도_y', '설치형태_전용주',\n",
              "       '설치형태_한전주', '설치개수', '어린이보호구역_위도', '어린이보호구역_경도', '보호구역도로폭', 'CCTV설치대수',\n",
              "       '초등학교개수', '주차장개수', '급지_1', '급지_2', '급지_3', '주차장_위도', '주차장_경도',\n",
              "       '주차장유형_노상', '주차장유형_노외'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drops = train.columns[train.columns.str.contains('위도')]\n",
        "train.drop(drops, axis = 1, inplace = True)\n",
        "test.drop(drops, axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "i5JLOvXNdFOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drops = train.columns[train.columns.str.contains('경도')]\n",
        "train.drop(drops, axis = 1, inplace = True)\n",
        "test.drop(drops, axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "ta2ruEP0eyjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drops = train.columns[train.columns.str.contains('요일_')]\n",
        "# train.drop(drops, axis = 1, inplace = True)\n",
        "# test.drop(drops, axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "iqL2MIWNgQlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drops = ['ID', '사고연도', '사고월', '설치연도_x', '설치연도_y', '사고일', '사고시간']\n",
        "train.drop(drops, axis = 1, inplace = True)\n",
        "test.drop(drops, axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "eFNbcg2VYOIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcuqCk0IkzTI",
        "outputId": "1003a99e-dd03-4502-836a-2ca932594450"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['요일', '기상상태', '시군구', '도로형태', '노면상태', '사고유형', 'ECLO', '구', '동', '사고요일',\n",
              "       'sin_사고시간', 'cos_사고시간', '계절', '사고주말여부', '시간대', '도로노선방향', '단속구분', '제한속도',\n",
              "       'CCTV개수', '설치형태_전용주', '설치형태_한전주', '설치개수', '보호구역도로폭', 'CCTV설치대수',\n",
              "       '초등학교개수', '주차장개수', '급지_1', '급지_2', '급지_3', '주차장유형_노상', '주차장유형_노외'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accident_counts = train['시군구'].value_counts().reset_index()\n",
        "accident_counts.columns = ['시군구', '사고횟수']\n",
        "\n",
        "train['사고발생횟수'] = train['시군구'].map(accident_counts.set_index('시군구')['사고횟수'])\n",
        "test['사고발생횟수'] = test['시군구'].map(accident_counts.set_index('시군구')['사고횟수'])"
      ],
      "metadata": {
        "id": "HeGMm6-hxeco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 범주형 특성\n",
        "sparse_features = ['요일', '기상상태', '시군구', '도로형태', '노면상태', '사고유형', '구', '동', '사고요일', '계절', '사고주말여부', '시간대', '도로노선방향', '단속구분', '제한속도', '보호구역도로폭']\n",
        "\n",
        "# 숫자형 특성\n",
        "dense_features = ['CCTV개수', '설치개수', 'CCTV설치대수', '설치형태_전용주', '설치형태_한전주', '초등학교개수', '주차장개수', '급지_1', '급지_2', '급지_3', '주차장유형_노상', '주차장유형_노외', '사고발생횟수']\n",
        "\n",
        "train[sparse_features] = train[sparse_features].fillna(-1)\n",
        "train[dense_features] = train[dense_features].fillna(0)\n",
        "test[sparse_features] = test[sparse_features].fillna(-1)\n",
        "test[dense_features] = test[dense_features].fillna(0)"
      ],
      "metadata": {
        "id": "R65d9my6I5Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in train.drop(['ECLO'], axis = 1).columns:\n",
        "  if (train[col].dtype == 'object'):\n",
        "    # tr_encoder = TargetEncoder(cols=[col])\n",
        "    # train[col] = tr_encoder.fit_transform(train[col], train['ECLO'])\n",
        "    # test[col] = tr_encoder.transform(test[col])\n",
        "    encoder = LabelEncoder()\n",
        "    train[col] = encoder.fit_transform(train[col])\n",
        "    test[col] = encoder.transform(test[col])\n",
        "  elif train[col].dtype == 'bool':\n",
        "    encoder = LabelEncoder()\n",
        "    train[col] = encoder.fit_transform(train[col])\n",
        "    test[col] = encoder.transform(test[col])"
      ],
      "metadata": {
        "id": "bI_NQ3eeXreA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = train.drop(['ECLO'], axis = 1)\n",
        "y = train['ECLO']\n",
        "X_test = test[X.columns]"
      ],
      "metadata": {
        "id": "XQtPXbu6ZGnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ-4m9MqlQXH",
        "outputId": "3a7f1fa3-02a4-46ac-ac47-e8dd766e12e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(39609, 32)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# skf = StratifiedKFold(n_splits = 9, shuffle = True, random_state = 42)\n",
        "# folds = []\n",
        "# models = {}\n",
        "# for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "#   print('='*10)\n",
        "#   print(f'Fold {fold}')\n",
        "#   print('='*10)\n",
        "#   X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
        "#   X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
        "#   model = CatBoostRegressor(random_state = 42) # tree_method = 'gpu_hist', gpu_id = 0\n",
        "#   model.fit(X_train, y_train, verbose = 0)\n",
        "\n",
        "#   y_pred = model.predict(X_val)\n",
        "#   y_pred = np.round(y_pred, 0).astype(int)\n",
        "#   #rmsle_scorer = rmsle(y_val, y_pred)\n",
        "#   rmse_scorer = rmse(y_val, y_pred)\n",
        "#   #print(f'RMSLE: {rmsle_scorer}')\n",
        "#   print(f'RMSE: {rmse_scorer}')\n",
        "#   models[fold] = model\n",
        "\n",
        "# for fold in range(9):\n",
        "#   sample_submission['ECLO'] += models[fold].predict(X_test) / 9\n",
        "\n",
        "# sample_submission.loc[sample_submission['ECLO'] < 0.0, 'ECLO'] = 0.0"
      ],
      "metadata": {
        "id": "_FE0rYiqUT75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample_submission.to_csv('./catboost.csv', index = False)"
      ],
      "metadata": {
        "id": "0cqSGyTQXVdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "automl"
      ],
      "metadata": {
        "id": "qN9oS5ORfV5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold"
      ],
      "metadata": {
        "id": "kyGiCYMpJWR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# kfold = KFold(n_splits = 3, shuffle = True, random_state = 42)\n",
        "\n",
        "# preds = 0\n",
        "# for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):\n",
        "#   print(fold)\n",
        "#   model = AutoML(mode=\"Compete\",\n",
        "#               algorithms = ['LightGBM', 'Xgboost', 'CatBoost'],\n",
        "#               n_jobs = -1, total_time_limit = 3600 * 5, eval_metric=\"rmse\", ml_task = \"regression\",)\n",
        "#   trainX, trainy = X.iloc[train_idx], y.iloc[train_idx]\n",
        "#   valX, valy = X.iloc[val_idx], y.iloc[val_idx]\n",
        "#   model.fit(trainX, trainy)\n",
        "#   sample_submission['ECLO'] += model.predict(X_test) / 3"
      ],
      "metadata": {
        "id": "lksdZ2IGI-cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fold_preds = np.sum(preds, 0) / 5"
      ],
      "metadata": {
        "id": "4eoWln7ILwVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "automl = AutoML(mode=\"Compete\",\n",
        "                algorithms = ['Random Forest', 'LightGBM', 'Xgboost', 'CatBoost'],\n",
        "                n_jobs = -1,total_time_limit=3600*5, eval_metric=\"rmse\", ml_task = \"regression\",)\n",
        "automl.fit(X, y)"
      ],
      "metadata": {
        "id": "hLw1I2I3cbfD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cce381d3-2934-4863-de00-2b38aeb28159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoML directory: AutoML_5\n",
            "The task is regression with evaluation metric rmse\n",
            "AutoML will use algorithms: ['Random Forest', 'LightGBM', 'Xgboost', 'CatBoost']\n",
            "AutoML will stack models\n",
            "AutoML will ensemble available models\n",
            "AutoML steps: ['adjust_validation', 'simple_algorithms', 'default_algorithms', 'not_so_random', 'golden_features', 'kmeans_features', 'insert_random_feature', 'features_selection', 'hill_climbing_1', 'hill_climbing_2', 'boost_on_errors', 'ensemble', 'stack', 'ensemble_stacked']\n",
            "* Step adjust_validation will try to check up to 1 model\n",
            "1_DecisionTree rmse 3.221812 trained in 0.95 seconds\n",
            "Adjust validation. Remove: 1_DecisionTree\n",
            "Validation strategy: 10-fold CV Shuffle\n",
            "Skip simple_algorithms because no parameters were generated.\n",
            "* Step default_algorithms will try to check up to 4 models\n",
            "1_Default_LightGBM rmse 3.243308 trained in 18.99 seconds\n",
            "There was an error during 2_Default_Xgboost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 3_Default_CatBoost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 4_Default_RandomForest training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "* Step not_so_random will try to check up to 36 models\n",
            "There was an error during 14_LightGBM training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 5_Xgboost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 23_CatBoost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 32_RandomForest training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 15_LightGBM training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 6_Xgboost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 24_CatBoost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 33_RandomForest training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 16_LightGBM training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 7_Xgboost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 25_CatBoost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 34_RandomForest training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 17_LightGBM training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 8_Xgboost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 26_CatBoost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 35_RandomForest training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 18_LightGBM training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 9_Xgboost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 27_CatBoost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 36_RandomForest training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 19_LightGBM training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 10_Xgboost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 28_CatBoost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 37_RandomForest training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 20_LightGBM training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 11_Xgboost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 29_CatBoost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 38_RandomForest training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 21_LightGBM training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 12_Xgboost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 30_CatBoost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 39_RandomForest training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 22_LightGBM training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 13_Xgboost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 31_CatBoost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 40_RandomForest training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "* Step golden_features will try to check up to 3 models\n",
            "There was an error during 10_Xgboost_GoldenFeatures training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 3_Default_CatBoost_GoldenFeatures training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 14_LightGBM_GoldenFeatures training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "* Step kmeans_features will try to check up to 3 models\n",
            "There was an error during 10_Xgboost_KMeansFeatures training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 3_Default_CatBoost_KMeansFeatures training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 14_LightGBM_KMeansFeatures training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "* Step insert_random_feature will try to check up to 1 model\n",
            "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.\n",
            "Problem during computing permutation importance. Skipping ...\n",
            "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.\n",
            "Problem during computing permutation importance. Skipping ...\n",
            "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.\n",
            "Problem during computing permutation importance. Skipping ...\n",
            "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.\n",
            "Problem during computing permutation importance. Skipping ...\n",
            "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.\n",
            "Problem during computing permutation importance. Skipping ...\n",
            "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.\n",
            "Problem during computing permutation importance. Skipping ...\n",
            "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.\n",
            "Problem during computing permutation importance. Skipping ...\n",
            "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.\n",
            "Problem during computing permutation importance. Skipping ...\n",
            "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.\n",
            "Problem during computing permutation importance. Skipping ...\n",
            "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.\n",
            "Problem during computing permutation importance. Skipping ...\n",
            "There was an error during 10_Xgboost_RandomFeature training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "Skip features_selection because no parameters were generated.\n",
            "* Step hill_climbing_1 will try to check up to 17 models\n",
            "There was an error during 41_Xgboost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 42_Xgboost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 43_CatBoost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 44_CatBoost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 45_LightGBM training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 46_LightGBM training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 47_LightGBM training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 48_Xgboost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 49_Xgboost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 50_CatBoost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 51_CatBoost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 52_CatBoost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 53_CatBoost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 54_LightGBM training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 55_RandomForest training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 56_RandomForest training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 57_RandomForest training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "* Step hill_climbing_2 will try to check up to 17 models\n",
            "There was an error during 58_Xgboost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 59_LightGBM training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 60_Xgboost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 61_Xgboost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 62_LightGBM training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 63_CatBoost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 64_CatBoost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 65_LightGBM training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 66_CatBoost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 67_CatBoost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 68_CatBoost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 69_CatBoost training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 70_RandomForest training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 71_RandomForest training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 72_RandomForest training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 73_RandomForest training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 74_RandomForest training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "* Step boost_on_errors will try to check up to 1 model\n",
            "There was an error during 42_Xgboost_BoostOnErrors training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "* Step ensemble will try to check up to 1 model\n",
            "There was an error during Ensemble training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "* Step stack will try to check up to 39 models\n",
            "There was an error during 42_Xgboost_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 45_LightGBM_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 3_Default_CatBoost_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 36_RandomForest_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 41_Xgboost_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 62_LightGBM_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 67_CatBoost_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 72_RandomForest_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 10_Xgboost_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 65_LightGBM_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 43_CatBoost_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 70_RandomForest_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 58_Xgboost_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 47_LightGBM_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 63_CatBoost_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 71_RandomForest_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 12_Xgboost_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 59_LightGBM_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 66_CatBoost_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 40_RandomForest_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 61_Xgboost_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 14_LightGBM_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 25_CatBoost_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 73_RandomForest_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 60_Xgboost_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 22_LightGBM_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 64_CatBoost_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 37_RandomForest_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 48_Xgboost_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 46_LightGBM_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 44_CatBoost_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 74_RandomForest_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 9_Xgboost_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 18_LightGBM_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 23_CatBoost_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 55_RandomForest_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 20_LightGBM_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 31_CatBoost_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "There was an error during 34_RandomForest_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n",
            "* Step ensemble_stacked will try to check up to 1 model\n",
            "There was an error during Ensemble_Stacked training.\n",
            "Please check AutoML_5/errors.md for details.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-7a4fd2ae8a7a>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                 \u001b[0malgorithms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Random Forest'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LightGBM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Xgboost'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CatBoost'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                 n_jobs = -1,total_time_limit=3600*5, eval_metric=\"rmse\", ml_task = \"regression\",)\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mautoml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/supervised/automl.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, cv, sensitive_features)\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0mAutoML\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mReturns\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mself\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         \"\"\"\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msensitive_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/supervised/base_automl.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, sample_weight, cv, sensitive_features)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/supervised/base_automl.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, sample_weight, cv, sensitive_features)\u001b[0m\n\u001b[1;32m   1224\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_level\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"finished\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1226\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_and_save_best\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_warnings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m             self.verbose_print(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/supervised/base_automl.py\u001b[0m in \u001b[0;36mselect_and_save_best\u001b[0;34m(self, show_warnings)\u001b[0m\n\u001b[1;32m   1391\u001b[0m                 \u001b[0mfout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"# AutoML Leaderboard\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m                 \u001b[0mfout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtabulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mldb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtablefmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pipe\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1393\u001b[0;31m                 LeaderboardPlots.compute(\n\u001b[0m\u001b[1;32m   1394\u001b[0m                     \u001b[0mldb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fairness_threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1395\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/supervised/utils/leaderboard_plots.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(ldb, model_path, fout, fairness_threshold)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# plt.title(\"\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboxplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36mboxplot_frame\u001b[0;34m(self, column, by, ax, fontsize, rot, grid, figsize, layout, return_type, backend, **kwargs)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_boxplot_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m def boxplot_frame(\n\u001b[0;32m--> 515\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m     \u001b[0mcolumn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m_get_plot_backend\u001b[0;34m(backend)\u001b[0m\n\u001b[1;32m   1884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m     \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1886\u001b[0;31m     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"pandas_plotting_backends\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1887\u001b[0m     \u001b[0;31m# entry_points lost dict API ~ PY 3.10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1888\u001b[0m     \u001b[0;31m# https://github.com/python/importlib_metadata/issues/298\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m_load_backend\u001b[0;34m(backend)\u001b[0m\n\u001b[1;32m   1815\u001b[0m             \u001b[0;34m:\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mfigs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m             \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1818\u001b[0m             >>> df = pd.DataFrame({'x': np.random.randn(n),\n\u001b[1;32m   1819\u001b[0m             ...                    'y': np.random.randn(n)})\n",
            "\u001b[0;31mImportError\u001b[0m: matplotlib is required for plotting when the default backend \"matplotlib\" is selected."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = automl.predict(X_test)\n",
        "sample_submission['ECLO'] = pred\n",
        "sample_submission.loc[sample_submission['ECLO'] < 0.0, 'ECLO'] = 0.0\n",
        "sample_submission.to_csv('./automl_fillna.csv', index = False)"
      ],
      "metadata": {
        "id": "vt8NUm4ifiKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission"
      ],
      "metadata": {
        "id": "7OMzRI_B9pAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "autogluon"
      ],
      "metadata": {
        "id": "CEvZPBUtwBEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = TabularDataset(train)\n",
        "test_data = TabularDataset(test)\n",
        "\n",
        "predictor = TabularPredictor(label='ECLO', eval_metric='rmse').fit(train_data, presets='high_quality') # , ag_args_fit={'num_gpus': 1}\n",
        "# predictor = TabularPredictor(label='ECLO',  eval_metric='rmse').fit(train_data, time_limit=43200, presets='high_quality',  ag_args_fit={'num_gpus': 1},\n",
        "#                                                                                      hyperparameters={'RF':{}, 'GBM':{},'XGB':{},'XT':{},'CAT':{}})\n",
        "\n",
        "pred = predictor.predict(test_data)\n",
        "sample_submission['ECLO'] = pred\n",
        "sample_submission.loc[sample_submission['ECLO'] < 0.0, 'ECLO'] = 0.0\n",
        "sample_submission.to_csv('./autogluon.csv', index = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYPoFVfxlcUb",
        "outputId": "b6b60b20-bb82-4dcb-de36-20442375dbf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20231210_143937\"\n",
            "Presets specified: ['high_quality']\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "Dynamic stacking is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
            "Detecting stacked overfitting by sub-fitting AutoGluon on the input data. That is, copies of AutoGluon will be sub-fit on subset(s) of the data. Then, the holdout validation data is used to detect stacked overfitting.\n",
            "Sub-fit(s) time limit is: 3600 seconds.\n",
            "Starting holdout-based sub-fit for dynamic stacking. Context path is: AutogluonModels/ag-20231210_143937/ds_sub_fit/sub_fit_ho.\n",
            "Beginning AutoGluon training ... Time limit = 900s\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20231210_143937/ds_sub_fit/sub_fit_ho\"\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.0.0\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Wed Aug 30 11:19:59 UTC 2023\n",
            "CPU Count:          2\n",
            "Memory Avail:       10.93 GB / 12.68 GB (86.2%)\n",
            "Disk Space Avail:   73.35 GB / 107.72 GB (68.1%)\n",
            "===================================================\n",
            "Train Data Rows:    35209\n",
            "Train Data Columns: 31\n",
            "Label Column:       ECLO\n",
            "Problem Type:       multiclass\n",
            "Preprocessing data ...\n",
            "Warning: Some classes in the training set have fewer than 10 examples. AutoGluon will only keep 26 out of 46 classes for training and will not try to predict the rare classes. To keep more classes, increase the number of datapoints from these rare classes in the training data or reduce label_count_threshold.\n",
            "Fraction of data from classes with at least 10 examples that will be kept for training models: 0.998778721349655\n",
            "Train Data Class Count: 26\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11194.74 MB\n",
            "\tTrain Data (Original)  Memory Usage: 8.32 MB (0.1% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 19 | ['sin_사고시간', 'cos_사고시간', '사고주말여부', '도로노선방향', '단속구분', ...]\n",
            "\t\t('int', [])   : 12 | ['요일', '기상상태', '시군구', '도로형태', '노면상태', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 18 | ['sin_사고시간', 'cos_사고시간', '도로노선방향', '단속구분', '제한속도', ...]\n",
            "\t\t('int', [])       : 12 | ['요일', '기상상태', '시군구', '도로형태', '노면상태', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['사고주말여부']\n",
            "\t0.6s = Fit runtime\n",
            "\t31 features in original data used to generate 31 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 8.08 MB (0.1% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.74s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 110 L1 models ...\n",
            "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 599.34s of the 899.19s of remaining time.\n",
            "\t-3.5275\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.16s\t = Training   runtime\n",
            "\t8.67s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 584.66s of the 884.51s of remaining time.\n",
            "\t-3.7895\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.12s\t = Training   runtime\n",
            "\t8.39s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 575.91s of the 875.76s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.84%)\n",
            "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=3306, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 854, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 354, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 119, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 264, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 199, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 253, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 199, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 248, in _do_epoch\n",
            "    self._do_epoch_validate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 244, in _do_epoch_validate\n",
            "    with torch.no_grad(): self._with_events(self.all_batches, 'validate', CancelValidException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 199, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 205, in all_batches\n",
            "    for o in enumerate(self.dl): self.one_batch(*o)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 235, in one_batch\n",
            "    self._with_events(self._do_one_batch, 'batch', CancelBatchException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 172, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 156, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 840, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 825, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 176, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 60, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 560, in after_batch\n",
            "    for met in mets: met.accumulate(self.learn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/metrics.py\", line 49, in accumulate\n",
            "    self.accum_values(pred,learn.y,learn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/metrics.py\", line 55, in accum_values\n",
            "    if self.flatten: preds,targs = flatten_check(preds,targs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/torch_core.py\", line 787, in flatten_check\n",
            "    test_eq(len(inp), len(targ))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/test.py\", line 37, in test_eq\n",
            "    test(a,b,equals, cname='==')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/test.py\", line 27, in test\n",
            "    assert cmp(a,b),f\"{cname}:\\n{a}\\n{b}\"\n",
            "AssertionError: Exception occured in `Recorder` when calling event `after_batch`:\n",
            "\t==:\n",
            "6656\n",
            "256\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1817, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1763, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 854, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 273, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 689, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 661, in after_all_folds_scheduled\n",
            "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 603, in _run_parallel\n",
            "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 566, in _process_fold_results\n",
            "    raise processed_exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 532, in _process_fold_results\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2524, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=3306, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 854, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 354, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 119, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 264, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 199, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 253, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 199, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 248, in _do_epoch\n",
            "    self._do_epoch_validate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 244, in _do_epoch_validate\n",
            "    with torch.no_grad(): self._with_events(self.all_batches, 'validate', CancelValidException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 199, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 205, in all_batches\n",
            "    for o in enumerate(self.dl): self.one_batch(*o)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 235, in one_batch\n",
            "    self._with_events(self._do_one_batch, 'batch', CancelBatchException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 172, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 156, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 840, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 825, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 176, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 60, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 560, in after_batch\n",
            "    for met in mets: met.accumulate(self.learn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/metrics.py\", line 49, in accumulate\n",
            "    self.accum_values(pred,learn.y,learn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/metrics.py\", line 55, in accum_values\n",
            "    if self.flatten: preds,targs = flatten_check(preds,targs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/torch_core.py\", line 787, in flatten_check\n",
            "    test_eq(len(inp), len(targ))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/test.py\", line 37, in test_eq\n",
            "    test(a,b,equals, cname='==')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/test.py\", line 27, in test\n",
            "    assert cmp(a,b),f\"{cname}:\\n{a}\\n{b}\"\n",
            "AssertionError: Exception occured in `Recorder` when calling event `after_batch`:\n",
            "\t==:\n",
            "6656\n",
            "256\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 554.31s of the 854.16s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=1.07%)\n",
            "2023-12-10 14:40:30,320\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t-3.5604\t = Validation score   (-root_mean_squared_error)\n",
            "\t310.09s\t = Training   runtime\n",
            "\t22.97s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 234.4s of the 534.25s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=1.05%)\n",
            "\t-3.6984\t = Validation score   (-root_mean_squared_error)\n",
            "\t203.9s\t = Training   runtime\n",
            "\t9.45s\t = Validation runtime\n",
            "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 23.19s of the 323.04s of remaining time.\n",
            "\tWarning: Reducing model 'n_estimators' from 300 -> 202 due to low memory. Expected memory usage reduced from 22.23% -> 15.0% of available memory...\n",
            "\tWarning: Reducing model 'n_estimators' from 202 -> 147 due to low time. Expected time usage reduced from 31.8s -> 23.2s...\n",
            "\t-3.7058\t = Validation score   (-root_mean_squared_error)\n",
            "\t14.62s\t = Training   runtime\n",
            "\t2.24s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 291.06s of remaining time.\n",
            "\tEnsemble Weights: {'KNeighborsUnif_BAG_L1': 0.625, 'LightGBM_BAG_L1': 0.203, 'LightGBMXT_BAG_L1': 0.156, 'RandomForestGini_BAG_L1': 0.016}\n",
            "\t-3.445\t = Validation score   (-root_mean_squared_error)\n",
            "\t12.69s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting 108 L2 models ...\n",
            "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 278.29s of the 278.15s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=2.67%)\n",
            "\tWarning: Exception caused NeuralNetFastAI_BAG_L2 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=6186, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 854, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 354, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 119, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 264, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 199, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 253, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 199, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 248, in _do_epoch\n",
            "    self._do_epoch_validate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 244, in _do_epoch_validate\n",
            "    with torch.no_grad(): self._with_events(self.all_batches, 'validate', CancelValidException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 199, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 205, in all_batches\n",
            "    for o in enumerate(self.dl): self.one_batch(*o)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 235, in one_batch\n",
            "    self._with_events(self._do_one_batch, 'batch', CancelBatchException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 172, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 156, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 840, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 825, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 176, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 60, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 560, in after_batch\n",
            "    for met in mets: met.accumulate(self.learn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/metrics.py\", line 49, in accumulate\n",
            "    self.accum_values(pred,learn.y,learn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/metrics.py\", line 55, in accum_values\n",
            "    if self.flatten: preds,targs = flatten_check(preds,targs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/torch_core.py\", line 787, in flatten_check\n",
            "    test_eq(len(inp), len(targ))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/test.py\", line 37, in test_eq\n",
            "    test(a,b,equals, cname='==')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/test.py\", line 27, in test\n",
            "    assert cmp(a,b),f\"{cname}:\\n{a}\\n{b}\"\n",
            "AssertionError: Exception occured in `Recorder` when calling event `after_batch`:\n",
            "\t==:\n",
            "6656\n",
            "256\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1817, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1763, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 854, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 273, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 689, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 661, in after_all_folds_scheduled\n",
            "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 603, in _run_parallel\n",
            "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 566, in _process_fold_results\n",
            "    raise processed_exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 532, in _process_fold_results\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2524, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=6186, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 854, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 354, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 119, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 264, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 199, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 253, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 199, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 248, in _do_epoch\n",
            "    self._do_epoch_validate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 244, in _do_epoch_validate\n",
            "    with torch.no_grad(): self._with_events(self.all_batches, 'validate', CancelValidException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 199, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 205, in all_batches\n",
            "    for o in enumerate(self.dl): self.one_batch(*o)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 235, in one_batch\n",
            "    self._with_events(self._do_one_batch, 'batch', CancelBatchException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 172, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 156, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 840, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 825, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 176, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 60, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 560, in after_batch\n",
            "    for met in mets: met.accumulate(self.learn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/metrics.py\", line 49, in accumulate\n",
            "    self.accum_values(pred,learn.y,learn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/metrics.py\", line 55, in accum_values\n",
            "    if self.flatten: preds,targs = flatten_check(preds,targs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/torch_core.py\", line 787, in flatten_check\n",
            "    test_eq(len(inp), len(targ))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/test.py\", line 37, in test_eq\n",
            "    test(a,b,equals, cname='==')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/test.py\", line 27, in test\n",
            "    assert cmp(a,b),f\"{cname}:\\n{a}\\n{b}\"\n",
            "AssertionError: Exception occured in `Recorder` when calling event `after_batch`:\n",
            "\t==:\n",
            "6656\n",
            "256\n",
            "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 262.67s of the 262.5s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=3.46%)\n",
            "2023-12-10 14:50:21,679\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t-3.672\t = Validation score   (-root_mean_squared_error)\n",
            "\t237.77s\t = Training   runtime\n",
            "\t2.55s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L2 ... Training model for up to 20.32s of the 20.16s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=3.40%)\n",
            "\t-3.9756\t = Validation score   (-root_mean_squared_error)\n",
            "\t43.89s\t = Training   runtime\n",
            "\t0.37s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the -35.37s of remaining time.\n",
            "\tEnsemble Weights: {'KNeighborsUnif_BAG_L1': 0.63, 'LightGBM_BAG_L2': 0.152, 'LightGBMXT_BAG_L2': 0.13, 'LightGBMXT_BAG_L1': 0.043, 'LightGBM_BAG_L1': 0.033, 'RandomForestGini_BAG_L1': 0.011}\n",
            "\t-3.4118\t = Validation score   (-root_mean_squared_error)\n",
            "\t14.24s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 949.79s ... Best model: \"WeightedEnsemble_L3\"\n",
            "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
            "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
            "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
            "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
            "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
            "Fitting model: KNeighborsUnif_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t0.16s\t = Training   runtime\n",
            "\t8.67s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t0.12s\t = Training   runtime\n",
            "\t8.39s\t = Validation runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
            "\t21.64s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: LightGBM_BAG_L1_FULL ...\n",
            "\t13.19s\t = Training   runtime\n",
            "Fitting model: RandomForestGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t14.62s\t = Training   runtime\n",
            "\t2.24s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
            "\tEnsemble Weights: {'KNeighborsUnif_BAG_L1': 0.625, 'LightGBM_BAG_L1': 0.203, 'LightGBMXT_BAG_L1': 0.156, 'RandomForestGini_BAG_L1': 0.016}\n",
            "\t12.69s\t = Training   runtime\n",
            "Fitting 1 L2 models ...\n",
            "Fitting model: LightGBMXT_BAG_L2_FULL ...\n",
            "\t13.96s\t = Training   runtime\n",
            "Fitting 1 L2 models ...\n",
            "Fitting model: LightGBM_BAG_L2_FULL ...\n",
            "\t2.35s\t = Training   runtime\n",
            "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
            "\tEnsemble Weights: {'KNeighborsUnif_BAG_L1': 0.63, 'LightGBM_BAG_L2': 0.152, 'LightGBMXT_BAG_L2': 0.13, 'LightGBMXT_BAG_L1': 0.043, 'LightGBM_BAG_L1': 0.033, 'RandomForestGini_BAG_L1': 0.011}\n",
            "\t14.24s\t = Training   runtime\n",
            "Updated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\n",
            "Refit complete, total runtime = 59.4s ... Best model: \"WeightedEnsemble_L3_FULL\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231210_143937/ds_sub_fit/sub_fit_ho\")\n",
            "Leaderboard on holdout data from dynamic stacking:\n",
            "                          model  holdout_score  score_val              eval_metric  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0      WeightedEnsemble_L3_FULL      -3.710407  -3.411798  root_mean_squared_error        8.606193            NaN  80.275821                 0.022126                     NaN          14.235647            3       True          9\n",
            "1      WeightedEnsemble_L2_FULL      -3.736685  -3.444984  root_mean_squared_error        6.804933            NaN  62.301253                 0.008479                     NaN          12.690296            2       True          6\n",
            "2          LightGBM_BAG_L1_FULL      -3.870962  -3.698380  root_mean_squared_error        0.887446            NaN  13.190592                 0.887446                     NaN          13.190592            1       True          4\n",
            "3    KNeighborsUnif_BAG_L1_FULL      -3.870962  -3.527522  root_mean_squared_error        1.395772       8.665809   0.162125                 1.395772                8.665809           0.162125            1       True          1\n",
            "4        LightGBMXT_BAG_L1_FULL      -4.017574  -3.560416  root_mean_squared_error        1.942518            NaN  21.636996                 1.942518                     NaN          21.636996            1       True          3\n",
            "5        LightGBMXT_BAG_L2_FULL      -4.050803  -3.672001  root_mean_squared_error        8.540334            NaN  63.687510                 0.224689                     NaN          13.958439            2       True          7\n",
            "6    KNeighborsDist_BAG_L1_FULL      -4.058525  -3.789464  root_mean_squared_error        1.519191       8.389198   0.118114                 1.519191                8.389198           0.118114            1       True          2\n",
            "7  RandomForestGini_BAG_L1_FULL      -4.179205  -3.705819  root_mean_squared_error        2.570718       2.236431  14.621246                 2.570718                2.236431          14.621246            1       True          5\n",
            "8          LightGBM_BAG_L2_FULL      -4.670556  -3.975645  root_mean_squared_error        8.359378            NaN  52.081736                 0.043733                     NaN           2.352664            2       True          8\n",
            "Stacked overfitting occurred: False.\n",
            "Spend 1020 seconds for the sub-fit(s) during dynamic stacking.\n",
            "Time left for full fit of AutoGluon: 2580 seconds.\n",
            "Starting full fit now with num_stack_levels 1.\n",
            "Beginning AutoGluon training ... Time limit = 2580s\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20231210_143937\"\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.0.0\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Wed Aug 30 11:19:59 UTC 2023\n",
            "CPU Count:          2\n",
            "Memory Avail:       10.26 GB / 12.68 GB (80.9%)\n",
            "Disk Space Avail:   73.35 GB / 107.72 GB (68.1%)\n",
            "===================================================\n",
            "Train Data Rows:    39609\n",
            "Train Data Columns: 31\n",
            "Label Column:       ECLO\n",
            "Problem Type:       multiclass\n",
            "Preprocessing data ...\n",
            "Warning: Some classes in the training set have fewer than 10 examples. AutoGluon will only keep 26 out of 46 classes for training and will not try to predict the rare classes. To keep more classes, increase the number of datapoints from these rare classes in the training data or reduce label_count_threshold.\n",
            "Fraction of data from classes with at least 10 examples that will be kept for training models: 0.9988386477820698\n",
            "Train Data Class Count: 26\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10517.97 MB\n",
            "\tTrain Data (Original)  Memory Usage: 9.36 MB (0.1% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 19 | ['sin_사고시간', 'cos_사고시간', '사고주말여부', '도로노선방향', '단속구분', ...]\n",
            "\t\t('int', [])   : 12 | ['요일', '기상상태', '시군구', '도로형태', '노면상태', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 18 | ['sin_사고시간', 'cos_사고시간', '도로노선방향', '단속구분', '제한속도', ...]\n",
            "\t\t('int', [])       : 12 | ['요일', '기상상태', '시군구', '도로형태', '노면상태', ...]\n",
            "\t\t('int', ['bool']) :  1 | ['사고주말여부']\n",
            "\t0.2s = Fit runtime\n",
            "\t31 features in original data used to generate 31 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 9.09 MB (0.1% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.31s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Fitting 110 L1 models ...\n",
            "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 1719.36s of the 2579.66s of remaining time.\n",
            "\t-3.535\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.09s\t = Training   runtime\n",
            "\t9.48s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 1709.59s of the 2569.9s of remaining time.\n",
            "\t-3.7837\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.11s\t = Training   runtime\n",
            "\t12.18s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1697.01s of the 2557.31s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.98%)\n",
            "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=8306, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 854, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 354, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 119, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 264, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 199, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 253, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 199, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 248, in _do_epoch\n",
            "    self._do_epoch_validate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 244, in _do_epoch_validate\n",
            "    with torch.no_grad(): self._with_events(self.all_batches, 'validate', CancelValidException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 199, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 205, in all_batches\n",
            "    for o in enumerate(self.dl): self.one_batch(*o)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 235, in one_batch\n",
            "    self._with_events(self._do_one_batch, 'batch', CancelBatchException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 172, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 156, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 840, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 825, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 176, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 60, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 560, in after_batch\n",
            "    for met in mets: met.accumulate(self.learn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/metrics.py\", line 49, in accumulate\n",
            "    self.accum_values(pred,learn.y,learn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/metrics.py\", line 55, in accum_values\n",
            "    if self.flatten: preds,targs = flatten_check(preds,targs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/torch_core.py\", line 787, in flatten_check\n",
            "    test_eq(len(inp), len(targ))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/test.py\", line 37, in test_eq\n",
            "    test(a,b,equals, cname='==')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/test.py\", line 27, in test\n",
            "    assert cmp(a,b),f\"{cname}:\\n{a}\\n{b}\"\n",
            "AssertionError: Exception occured in `Recorder` when calling event `after_batch`:\n",
            "\t==:\n",
            "6656\n",
            "256\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1817, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1763, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 854, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 273, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 689, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 661, in after_all_folds_scheduled\n",
            "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 603, in _run_parallel\n",
            "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 566, in _process_fold_results\n",
            "    raise processed_exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 532, in _process_fold_results\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2524, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=8306, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 854, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 354, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 119, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 264, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 199, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 253, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 199, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 248, in _do_epoch\n",
            "    self._do_epoch_validate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 244, in _do_epoch_validate\n",
            "    with torch.no_grad(): self._with_events(self.all_batches, 'validate', CancelValidException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 199, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 205, in all_batches\n",
            "    for o in enumerate(self.dl): self.one_batch(*o)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 235, in one_batch\n",
            "    self._with_events(self._do_one_batch, 'batch', CancelBatchException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 172, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 156, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 840, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 825, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 176, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 60, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 560, in after_batch\n",
            "    for met in mets: met.accumulate(self.learn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/metrics.py\", line 49, in accumulate\n",
            "    self.accum_values(pred,learn.y,learn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/metrics.py\", line 55, in accum_values\n",
            "    if self.flatten: preds,targs = flatten_check(preds,targs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/torch_core.py\", line 787, in flatten_check\n",
            "    test_eq(len(inp), len(targ))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/test.py\", line 37, in test_eq\n",
            "    test(a,b,equals, cname='==')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/test.py\", line 27, in test\n",
            "    assert cmp(a,b),f\"{cname}:\\n{a}\\n{b}\"\n",
            "AssertionError: Exception occured in `Recorder` when calling event `after_batch`:\n",
            "\t==:\n",
            "6656\n",
            "256\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1682.19s of the 2542.49s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=1.24%)\n",
            "2023-12-10 14:57:20,920\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t-3.5203\t = Validation score   (-root_mean_squared_error)\n",
            "\t262.92s\t = Training   runtime\n",
            "\t12.65s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 1414.15s of the 2274.46s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=1.20%)\n",
            "\t-4.6437\t = Validation score   (-root_mean_squared_error)\n",
            "\t268.01s\t = Training   runtime\n",
            "\t14.51s\t = Validation runtime\n",
            "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 1136.26s of the 1996.57s of remaining time.\n",
            "\tWarning: Reducing model 'n_estimators' from 300 -> 197 due to low memory. Expected memory usage reduced from 22.83% -> 15.0% of available memory...\n",
            "\t-3.6856\t = Validation score   (-root_mean_squared_error)\n",
            "\t24.25s\t = Training   runtime\n",
            "\t2.7s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 1100.48s of the 1960.79s of remaining time.\n",
            "\tWarning: Reducing model 'n_estimators' from 300 -> 202 due to low memory. Expected memory usage reduced from 22.27% -> 15.0% of available memory...\n",
            "\t-3.7116\t = Validation score   (-root_mean_squared_error)\n",
            "\t26.98s\t = Training   runtime\n",
            "\t2.71s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L1 ... Training model for up to 1059.61s of the 1919.91s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=1.25%)\n",
            "\t-3.4434\t = Validation score   (-root_mean_squared_error)\n",
            "\t867.65s\t = Training   runtime\n",
            "\t1.46s\t = Validation runtime\n",
            "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 186.26s of the 1046.57s of remaining time.\n",
            "\tWarning: Reducing model 'n_estimators' from 300 -> 189 due to low memory. Expected memory usage reduced from 23.7% -> 15.0% of available memory...\n",
            "\t-3.6589\t = Validation score   (-root_mean_squared_error)\n",
            "\t17.04s\t = Training   runtime\n",
            "\t2.51s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 148.85s of the 1009.16s of remaining time.\n",
            "\tWarning: Reducing model 'n_estimators' from 300 -> 197 due to low memory. Expected memory usage reduced from 22.81% -> 15.0% of available memory...\n",
            "\t-3.6701\t = Validation score   (-root_mean_squared_error)\n",
            "\t17.24s\t = Training   runtime\n",
            "\t2.5s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L1 ... Training model for up to 112.8s of the 973.11s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=1.45%)\n",
            "\t-3.4366\t = Validation score   (-root_mean_squared_error)\n",
            "\t108.84s\t = Training   runtime\n",
            "\t2.84s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 855.2s of remaining time.\n",
            "\tEnsemble Weights: {'XGBoost_BAG_L1': 0.704, 'KNeighborsUnif_BAG_L1': 0.13, 'CatBoost_BAG_L1': 0.093, 'ExtraTreesGini_BAG_L1': 0.037, 'RandomForestGini_BAG_L1': 0.019, 'ExtraTreesEntr_BAG_L1': 0.019}\n",
            "\t-3.427\t = Validation score   (-root_mean_squared_error)\n",
            "\t19.09s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting 108 L2 models ...\n",
            "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 835.94s of the 835.57s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=5.25%)\n",
            "\tWarning: Exception caused NeuralNetFastAI_BAG_L2 to fail during training... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=16145, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 854, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 354, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 119, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 264, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 199, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 253, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 199, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 248, in _do_epoch\n",
            "    self._do_epoch_validate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 244, in _do_epoch_validate\n",
            "    with torch.no_grad(): self._with_events(self.all_batches, 'validate', CancelValidException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 199, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 205, in all_batches\n",
            "    for o in enumerate(self.dl): self.one_batch(*o)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 235, in one_batch\n",
            "    self._with_events(self._do_one_batch, 'batch', CancelBatchException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 172, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 156, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 840, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 825, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 176, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 60, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 560, in after_batch\n",
            "    for met in mets: met.accumulate(self.learn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/metrics.py\", line 49, in accumulate\n",
            "    self.accum_values(pred,learn.y,learn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/metrics.py\", line 55, in accum_values\n",
            "    if self.flatten: preds,targs = flatten_check(preds,targs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/torch_core.py\", line 787, in flatten_check\n",
            "    test_eq(len(inp), len(targ))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/test.py\", line 37, in test_eq\n",
            "    test(a,b,equals, cname='==')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/test.py\", line 27, in test\n",
            "    assert cmp(a,b),f\"{cname}:\\n{a}\\n{b}\"\n",
            "AssertionError: Exception occured in `Recorder` when calling event `after_batch`:\n",
            "\t==:\n",
            "6656\n",
            "256\n",
            "Detailed Traceback:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1817, in _train_and_save\n",
            "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1763, in _train_single\n",
            "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 854, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
            "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 273, in _fit\n",
            "    self._fit_folds(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 689, in _fit_folds\n",
            "    fold_fitting_strategy.after_all_folds_scheduled()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 661, in after_all_folds_scheduled\n",
            "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 603, in _run_parallel\n",
            "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 566, in _process_fold_results\n",
            "    raise processed_exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 532, in _process_fold_results\n",
            "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2524, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=16145, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 854, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 354, in _fit\n",
            "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/schedule.py\", line 119, in fit_one_cycle\n",
            "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 264, in fit\n",
            "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 199, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 253, in _do_fit\n",
            "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 199, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 248, in _do_epoch\n",
            "    self._do_epoch_validate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 244, in _do_epoch_validate\n",
            "    with torch.no_grad(): self._with_events(self.all_batches, 'validate', CancelValidException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 199, in _with_events\n",
            "    try: self(f'before_{event_type}');  f()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 205, in all_batches\n",
            "    for o in enumerate(self.dl): self.one_batch(*o)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 235, in one_batch\n",
            "    self._with_events(self._do_one_batch, 'batch', CancelBatchException)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 201, in _with_events\n",
            "    self(f'after_{event_type}');  final()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 172, in __call__\n",
            "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/foundation.py\", line 156, in map\n",
            "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 840, in map_ex\n",
            "    return list(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/basics.py\", line 825, in __call__\n",
            "    return self.func(*fargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 176, in _call_one\n",
            "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 62, in __call__\n",
            "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/callback/core.py\", line 60, in __call__\n",
            "    try: res = getcallable(self, event_name)()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/learner.py\", line 560, in after_batch\n",
            "    for met in mets: met.accumulate(self.learn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/metrics.py\", line 49, in accumulate\n",
            "    self.accum_values(pred,learn.y,learn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/metrics.py\", line 55, in accum_values\n",
            "    if self.flatten: preds,targs = flatten_check(preds,targs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastai/torch_core.py\", line 787, in flatten_check\n",
            "    test_eq(len(inp), len(targ))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/test.py\", line 37, in test_eq\n",
            "    test(a,b,equals, cname='==')\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fastcore/test.py\", line 27, in test\n",
            "    assert cmp(a,b),f\"{cname}:\\n{a}\\n{b}\"\n",
            "AssertionError: Exception occured in `Recorder` when calling event `after_batch`:\n",
            "\t==:\n",
            "6656\n",
            "256\n",
            "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 816.06s of the 815.54s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=6.82%)\n",
            "2023-12-10 15:26:06,570\tERROR worker.py:405 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
            "\t-3.7448\t = Validation score   (-root_mean_squared_error)\n",
            "\t692.24s\t = Training   runtime\n",
            "\t10.72s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L2 ... Training model for up to 114.34s of the 113.94s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=6.71%)\n",
            "\t-4.1265\t = Validation score   (-root_mean_squared_error)\n",
            "\t129.59s\t = Training   runtime\n",
            "\t0.87s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the -30.24s of remaining time.\n",
            "\tEnsemble Weights: {'XGBoost_BAG_L1': 0.689, 'KNeighborsUnif_BAG_L1': 0.133, 'CatBoost_BAG_L1': 0.078, 'ExtraTreesGini_BAG_L1': 0.033, 'RandomForestGini_BAG_L1': 0.022, 'ExtraTreesEntr_BAG_L1': 0.022, 'LightGBMXT_BAG_L2': 0.022}\n",
            "\t-3.4269\t = Validation score   (-root_mean_squared_error)\n",
            "\t22.43s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 2632.88s ... Best model: \"WeightedEnsemble_L3\"\n",
            "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
            "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
            "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
            "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
            "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
            "Fitting model: KNeighborsUnif_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t0.09s\t = Training   runtime\n",
            "\t9.48s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t0.11s\t = Training   runtime\n",
            "\t12.18s\t = Validation runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
            "\t20.26s\t = Training   runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: LightGBM_BAG_L1_FULL ...\n",
            "\t17.82s\t = Training   runtime\n",
            "Fitting model: RandomForestGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t24.25s\t = Training   runtime\n",
            "\t2.7s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t26.98s\t = Training   runtime\n",
            "\t2.71s\t = Validation runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: CatBoost_BAG_L1_FULL ...\n",
            "\t106.17s\t = Training   runtime\n",
            "Fitting model: ExtraTreesGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t17.04s\t = Training   runtime\n",
            "\t2.51s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t17.24s\t = Training   runtime\n",
            "\t2.5s\t = Validation runtime\n",
            "Fitting 1 L1 models ...\n",
            "Fitting model: XGBoost_BAG_L1_FULL ...\n",
            "\t4.98s\t = Training   runtime\n",
            "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
            "\tEnsemble Weights: {'XGBoost_BAG_L1': 0.704, 'KNeighborsUnif_BAG_L1': 0.13, 'CatBoost_BAG_L1': 0.093, 'ExtraTreesGini_BAG_L1': 0.037, 'RandomForestGini_BAG_L1': 0.019, 'ExtraTreesEntr_BAG_L1': 0.019}\n",
            "\t19.09s\t = Training   runtime\n",
            "Fitting 1 L2 models ...\n",
            "Fitting model: LightGBMXT_BAG_L2_FULL ...\n",
            "\t54.0s\t = Training   runtime\n",
            "Fitting 1 L2 models ...\n",
            "Fitting model: LightGBM_BAG_L2_FULL ...\n",
            "\t8.82s\t = Training   runtime\n",
            "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
            "\tEnsemble Weights: {'XGBoost_BAG_L1': 0.689, 'KNeighborsUnif_BAG_L1': 0.133, 'CatBoost_BAG_L1': 0.078, 'ExtraTreesGini_BAG_L1': 0.033, 'RandomForestGini_BAG_L1': 0.022, 'ExtraTreesEntr_BAG_L1': 0.022, 'LightGBMXT_BAG_L2': 0.022}\n",
            "\t22.43s\t = Training   runtime\n",
            "Updated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\n",
            "Refit complete, total runtime = 279.25s ... Best model: \"WeightedEnsemble_L3_FULL\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20231210_143937\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "4T9SQYEWJOJh",
        "outputId": "ab5cc743-865d-4bd4-a26b-c14c8b6f54a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   ID  ECLO\n",
              "0      ACCIDENT_39609     3\n",
              "1      ACCIDENT_39610     3\n",
              "2      ACCIDENT_39611     3\n",
              "3      ACCIDENT_39612     3\n",
              "4      ACCIDENT_39613     3\n",
              "...               ...   ...\n",
              "10958  ACCIDENT_50567     3\n",
              "10959  ACCIDENT_50568     3\n",
              "10960  ACCIDENT_50569     3\n",
              "10961  ACCIDENT_50570     3\n",
              "10962  ACCIDENT_50571     3\n",
              "\n",
              "[10963 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3cc32a2f-839b-4892-a15b-62bf539c6f70\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>ECLO</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ACCIDENT_39609</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ACCIDENT_39610</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ACCIDENT_39611</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ACCIDENT_39612</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ACCIDENT_39613</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10958</th>\n",
              "      <td>ACCIDENT_50567</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10959</th>\n",
              "      <td>ACCIDENT_50568</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10960</th>\n",
              "      <td>ACCIDENT_50569</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10961</th>\n",
              "      <td>ACCIDENT_50570</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10962</th>\n",
              "      <td>ACCIDENT_50571</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10963 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3cc32a2f-839b-4892-a15b-62bf539c6f70')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3cc32a2f-839b-4892-a15b-62bf539c6f70 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3cc32a2f-839b-4892-a15b-62bf539c6f70');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8003e2a8-1271-4973-a156-307a100d7452\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8003e2a8-1271-4973-a156-307a100d7452')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8003e2a8-1271-4973-a156-307a100d7452 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "deppfm"
      ],
      "metadata": {
        "id": "vqs2S8ma-PM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade deepctr-torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q_eLHmTm-Q3D",
        "outputId": "d2b425e1-e817-4e5a-9d2f-da326261680e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deepctr-torch in /usr/local/lib/python3.10/dist-packages (0.2.9)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from deepctr-torch) (2.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepctr-torch) (4.65.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from deepctr-torch) (1.3.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from deepctr-torch) (2.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->deepctr-torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->deepctr-torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->deepctr-torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->deepctr-torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->deepctr-torch) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->deepctr-torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->deepctr-torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->deepctr-torch) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->deepctr-torch) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->deepctr-torch) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->deepctr-torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->deepctr-torch) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->deepctr-torch) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->deepctr-torch) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->deepctr-torch) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->deepctr-torch) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.2.0->deepctr-torch) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.2.0->deepctr-torch) (60.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.2.0->deepctr-torch) (0.42.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.2.0->deepctr-torch) (3.27.7)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.2.0->deepctr-torch) (17.0.6)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->deepctr-torch) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->deepctr-torch) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->deepctr-torch) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->deepctr-torch) (3.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->deepctr-torch) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->deepctr-torch) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow->deepctr-torch) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->deepctr-torch) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->deepctr-torch) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->deepctr-torch) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->deepctr-torch) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->deepctr-torch) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->deepctr-torch) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->deepctr-torch) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->deepctr-torch) (3.20.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->deepctr-torch) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->deepctr-torch) (2.3.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->deepctr-torch) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->deepctr-torch) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->deepctr-torch) (1.59.3)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow->deepctr-torch) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->deepctr-torch) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->deepctr-torch) (2.14.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->deepctr-torch) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->deepctr-torch) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->deepctr-torch) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->deepctr-torch) (2.28.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->deepctr-torch) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow->deepctr-torch) (3.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.2.0->deepctr-torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.2.0->deepctr-torch) (1.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->deepctr-torch) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->deepctr-torch) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->deepctr-torch) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->deepctr-torch) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->deepctr-torch) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->deepctr-torch) (3.6)\n",
            "Collecting urllib3<1.27,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->deepctr-torch)\n",
            "  Using cached urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow->deepctr-torch) (2023.11.17)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow->deepctr-torch) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow->deepctr-torch) (3.2.2)\n",
            "Installing collected packages: urllib3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.1.0\n",
            "    Uninstalling urllib3-2.1.0:\n",
            "      Successfully uninstalled urllib3-2.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "yfinance 0.2.32 requires requests>=2.31, but you have requests 2.28.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed urllib3-1.26.18\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade urllib3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ee2rDi1s9Owz",
        "outputId": "b7cd5281-75c7-4633-afde-8db3b65a6fe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (1.26.18)\n",
            "Collecting urllib3\n",
            "  Downloading urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: urllib3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.18\n",
            "    Uninstalling urllib3-1.26.18:\n",
            "      Successfully uninstalled urllib3-1.26.18\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "botocore 1.33.11 requires urllib3<2.1,>=1.25.4; python_version >= \"3.10\", but you have urllib3 2.1.0 which is incompatible.\n",
            "requests 2.28.2 requires urllib3<1.27,>=1.21.1, but you have urllib3 2.1.0 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "yfinance 0.2.32 requires requests>=2.31, but you have requests 2.28.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed urllib3-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from deepctr_torch.inputs import SparseFeat, DenseFeat, get_feature_names\n",
        "from deepctr_torch.models import *\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "riWOwNXBY73W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 요일, 기상상태, 시군구, 도로형태, 노면상태, 사고유형, 구, 동, 사고요일, 계절, 사고주말여부, 시간대, 도로노선방향, 단속구분, 제한속도, 보호구역도로폭\n",
        "# CCTV개수, 설치개수, CCTV설치대수, 초등학교개수, 주차장개수, 급지_1, 급지_2, 급지_3, 주차장유형_노상, 주차장유형_노외, 사고발생횟수\n",
        "\n",
        "# 범주형 특성\n",
        "sparse_features = ['요일', '기상상태', '시군구', '도로형태', '노면상태', '사고유형', '구', '동', '사고요일', '계절', '사고주말여부', '시간대', '도로노선방향', '단속구분', '제한속도', '보호구역도로폭']\n",
        "\n",
        "# 숫자형 특성\n",
        "dense_features = ['CCTV개수', '설치개수', 'CCTV설치대수', '설치형태_전용주', '설치형태_한전주', '초등학교개수', '주차장개수', '급지_1', '급지_2', '급지_3', '주차장유형_노상', '주차장유형_노외', '사고발생횟수']\n",
        "\n",
        "train[sparse_features] = train[sparse_features].fillna(-1)\n",
        "train[dense_features] = train[dense_features].fillna(0)\n",
        "test[sparse_features] = test[sparse_features].fillna(-1)\n",
        "test[dense_features] = test[dense_features].fillna(0)\n",
        "target = 'ECLO'\n",
        "\n",
        "# for feat in sparse_features:\n",
        "#   encoder = LabelEncoder()\n",
        "#   train[feat] = encoder.fit_transform(train[feat])\n",
        "#   test[feat] = encoder.transform(test[feat])\n",
        "\n",
        "scaler = MinMaxScaler(feature_range = (0, 1))\n",
        "train[dense_features] = scaler.fit_transform(train[dense_features])\n",
        "test[dense_features] = scaler.fit_transform(test[dense_features])"
      ],
      "metadata": {
        "id": "O_fZcrgA_X_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fixlen_feature_columns = [SparseFeat(feat, train[feat].nunique()) for feat in sparse_features] + [DenseFeat(feat, 1, ) for feat in dense_features]\n",
        "\n",
        "dnn_feature_columns = fixlen_feature_columns\n",
        "linear_feature_columns = fixlen_feature_columns\n",
        "\n",
        "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)"
      ],
      "metadata": {
        "id": "FAvqNkH3_a7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val = train_test_split(train, test_size = 0.2, random_state = 42)\n",
        "\n",
        "train_model_input = {name: train[name] for name in feature_names}\n",
        "val_model_input = {name: X_val[name] for name in feature_names}\n",
        "test_model_input = {name: test[name] for name in feature_names}"
      ],
      "metadata": {
        "id": "eFKowvv7_7_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cpu'\n",
        "use_cuda = True\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    print('cuda')\n",
        "    device = 'cuda:0'\n",
        "\n",
        "batch_size = 256\n",
        "epoch = 10\n",
        "verbose = 2\n",
        "\n",
        "model = DeepFM(linear_feature_columns=linear_feature_columns, dnn_feature_columns=dnn_feature_columns, task='regression', device=device)\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
        "\n",
        "model.fit(train_model_input, train[target].values, batch_size = batch_size, epoch = epoch, verbose = verbose, vaildation_split = 0.2)\n",
        "val_pred = model.predict(val_model_input, 256)\n",
        "print('Test MSE: ', round(mean_squared_error(X_val[target].values, val_pred), 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "XeAmAVZxZWFx",
        "outputId": "8cb337e9-4f51-4a36-d182-c1c8e0beb3ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-876-653c6d6e79ed>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Compile the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_model_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaildation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deepctr_torch/models/basemodel.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics)\u001b[0m\n\u001b[1;32m    441\u001b[0m         \"\"\"\n\u001b[1;32m    442\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_optim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_loss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/deepctr_torch/models/basemodel.py\u001b[0m in \u001b[0;36m_get_optim\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m                 \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"adagrad\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m                 \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdagrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     43\u001b[0m             if not all(\n\u001b[1;32m     44\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cuda\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mpg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             ):\n\u001b[1;32m     47\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`fused=True` requires all the params to be CUDA, floating point Tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mallowed_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlist_backends\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconvert_frame\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m from .eval_frame import (\n\u001b[1;32m      5\u001b[0m     \u001b[0massume_constant_result\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mguards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCheckFunctionManager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGuardedCode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moutput_graph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOutputGraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mreplay_record\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExecutionRecord\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msymbolic_convert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInstructionTranslator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_shapes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mShapeEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtorchdynamo_logging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCompiledFn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompilerFn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbytecode_transformation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_instruction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInstruction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariableTracker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbuiltin\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBuiltinVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConstantVariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEnumVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdicts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConstDictVariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataClassVariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDefaultDictVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m from .functions import (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munimplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAttrSource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdict_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mistype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0modict_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/source.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdataclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mLocalSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mlocal_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/dataclasses.py\u001b[0m in \u001b[0;36mdataclass\u001b[0;34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m     \u001b[0;31m# We're called as @dataclass without parens.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/dataclasses.py\u001b[0m in \u001b[0;36mwrap\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m         return _process_class(cls, init, repr, eq, order, unsafe_hash,\n\u001b[0m\u001b[1;32m   1176\u001b[0m                               frozen, match_args, kw_only, slots)\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/dataclasses.py\u001b[0m in \u001b[0;36m_process_class\u001b[0;34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0;31m# Raise an exception if any of our bases are frozen, but we're not.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many_frozen_base\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfrozen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             raise TypeError('cannot inherit non-frozen dataclass from a '\n\u001b[0m\u001b[1;32m    986\u001b[0m                             'frozen one')\n\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot inherit non-frozen dataclass from a frozen one"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xo5du4ze-m9P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}